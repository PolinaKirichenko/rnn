{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import time\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist_images(filename):\n",
    "    # Read the inputs in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8, offset=16)\n",
    "    # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "    # following the shape convention: (examples, channels, rows, columns)\n",
    "    data = data.reshape(-1, 28, 28)\n",
    "    # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "    # (Actually to range [0, 255/256], for compatibility to the version\n",
    "    # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "    return data\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    # Read the labels in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    # The labels are vectors of integers now, that's exactly what we want.\n",
    "    return data\n",
    "\n",
    "# We can now download and read the training and test set images and labels.\n",
    "X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr4AAABZCAYAAADYWSdmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZAUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f011b2bb208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((60000, 784))\n",
    "X_test = X_test.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.zeros((y_train.size, 10))\n",
    "a[np.arange(y_train.size), y_train] = 1\n",
    "y_train = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.zeros((y_test.size, 10))\n",
    "a[np.arange(y_test.size), y_test] = 1\n",
    "y_test = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "GRAD_CLIP = 10\n",
    "N_HIDDEN = 100\n",
    "SEQ_LEN = 784\n",
    "TRAIN_SIZE = 60000\n",
    "TEST_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numerically stable log-softmax with crossentropy\n",
    "def logsoftmax(x):\n",
    "    xdev = x-x.max(1,keepdims=True)\n",
    "    lsm = xdev - T.log(T.sum(T.exp(xdev),axis=1,keepdims=True))\n",
    "    return lsm\n",
    "\n",
    "# cross-entropy\n",
    "# ys are indices of chars, x is a matrix (? BATCH_SIZE * SEQ_LEN, VOCAB_SIZE)\n",
    "def lsmCE(x,y):\n",
    "    print(T.shape(x), T.shape(y))\n",
    "    return -T.clip(x,-20,0)[T.arange(y.shape[0]), y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Технические вещи\n",
    "\n",
    "# Вспомогательная функция для запаковки результата обучения \n",
    "def pack(train_err, train_acc, test_err,test_acc, network, inp, target,train_fn, test_fn):\n",
    "    return {'train_err':train_err, \n",
    "        'train_acc':train_acc, \n",
    "        'test_err':test_err, \n",
    "        'test_acc':test_acc, \n",
    "        'network':network,\n",
    "        'inp':inp,\n",
    "        'target':target,\n",
    "        'train_fn':train_fn, \n",
    "        'test_fn':test_fn\n",
    "           } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_posdef_w():\n",
    "    R = np.random.normal(size=(N_HIDDEN, N_HIDDEN))\n",
    "    A = 1 / N_HIDDEN * np.dot(R.T, R)\n",
    "    eig, _ = np.linalg.eig(A + np.eye(N_HIDDEN))\n",
    "    e = max(eig)\n",
    "    W = (A + np.eye(N_HIDDEN)) / e\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(inp, num_epochs=NUM_EPOCHS):\n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, max sequence length, number of features)\n",
    "    l_in = lasagne.layers.InputLayer(shape=(BATCH_SIZE, SEQ_LEN, 1), input_var=inp)\n",
    "\n",
    "    alpha = np.sqrt(2) * np.exp(1.2 / (max(N_HIDDEN, 6)))\n",
    "    # We're using a bidirectional network, which means we will combine two\n",
    "    # RecurrentLayers, one with the backwards=True keyword argument.\n",
    "    # Setting a value for grad_clipping will clip the gradients in the layer\n",
    "    l_rnn = lasagne.layers.RecurrentLayer(\n",
    "        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        W_in_to_hid=lasagne.init.Normal(std=alpha / N_HIDDEN, mean=0.0),\n",
    "        W_hid_to_hid=init_posdef_w(),\n",
    "        learn_init=True,\n",
    "        only_return_final = True,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    l_shp = lasagne.layers.ReshapeLayer(l_rnn, (-1, N_HIDDEN))\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(l_shp, num_units=10,\n",
    "                                      W=lasagne.init.GlorotNormal(),\n",
    "                                      nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(Xtrain, ytrain, Xtest, ytest, num_epochs=NUM_EPOCHS):\n",
    "    print(\"Building network ...\")\n",
    "    inp = T.tensor3('input')\n",
    "    target_values = T.imatrix('target_output')\n",
    "    network = build_network(inp)\n",
    "    print(\"The network has {} params\".format(lasagne.layers.count_params(network)))\n",
    "    \n",
    "    train_size = len(ytrain)\n",
    "    test_size = len(ytest)\n",
    "    num_train_batches = train_size // BATCH_SIZE\n",
    "    num_test_batches = test_size // BATCH_SIZE\n",
    "    train_err=np.zeros(NUM_EPOCHS)\n",
    "    train_acc=np.zeros(NUM_EPOCHS)\n",
    "    test_err=np.zeros(NUM_EPOCHS)\n",
    "    test_acc=np.zeros(NUM_EPOCHS)\n",
    "    \n",
    "    # lasagne.layers.get_output produces a variable for the output of the net\n",
    "    network_output = lasagne.layers.get_output(network)\n",
    "    # The value we care about is the final value produced for each sequence\n",
    "    # first dim of predicted_values is BATCH_SIZE\n",
    "    # Our cost will be mean-squared error\n",
    "    cost = T.mean(lasagne.objectives.categorical_crossentropy(network_output, target_values))\n",
    "    # Retrieve all parameters from the network\n",
    "    all_params = lasagne.layers.get_all_params(network)\n",
    "    # Compute SGD updates for training\n",
    "    print(\"Computing updates ...\")\n",
    "    all_grads = T.grad(cost, all_params)\n",
    "    scaled_grads = lasagne.updates.total_norm_constraint(all_grads, GRAD_CLIP)\n",
    "    lr_var = theano.shared(1e-4)\n",
    "    updates = lasagne.updates.sgd(scaled_grads, all_params, learning_rate=lr_var)\n",
    "    # Theano functions for training and computing cost\n",
    "    print(\"Compiling functions ...\")\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    acc_score = (test_prediction * target_values).sum()/1./target_values.shape[0]*100\n",
    "    \n",
    "    train_fn = theano.function([inp, target_values], [cost, acc_score], updates=updates, allow_input_downcast=True)\n",
    "    test_fn = theano.function([inp, target_values], [cost, acc_score], allow_input_downcast=True)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        if epoch == 33:\n",
    "            lr_var.set_value(1e-5)\n",
    "        if epoch == 66:\n",
    "            lr_var.set_value(1e-6)\n",
    "        for batch in range(num_train_batches):\n",
    "            idx = np.random.randint(TRAIN_SIZE, size=BATCH_SIZE)\n",
    "            err, acc = train_fn(Xtrain[idx, :, np.newaxis], ytrain[idx])\n",
    "            train_err[epoch] += err\n",
    "            train_acc[epoch] += acc\n",
    "        train_err[epoch] /= num_train_batches\n",
    "        train_acc[epoch] /= num_train_batches\n",
    "        \n",
    "        terr, tacc = test_fn(Xtest[:, :, np.newaxis], ytest)\n",
    "        test_err[epoch] = terr\n",
    "        test_acc[epoch] = tacc\n",
    "        \n",
    "        print(\"Epoch {} \\t loss / accuracy test = {:.4f}, {:.4f} \\t train = {:.4f}, {:.4f} \\t time = {:.2f}s\".\n",
    "              format(epoch, test_err[epoch], test_acc[epoch], \n",
    "                     train_err[epoch], train_acc[epoch],time.time() - start_time))\n",
    "    return pack(train_err, train_acc, test_err, test_acc,\n",
    "                network, inp, target_values, train_fn, test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "The network has 11310 params\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Epoch 0 \t loss / accuracy test = 2.3018, 10.0085 \t train = 2.3020, 10.0063 \t time = 1807.15s\n",
      "Epoch 1 \t loss / accuracy test = 2.3013, 10.0142 \t train = 2.3015, 10.0119 \t time = 1796.16s\n",
      "Epoch 2 \t loss / accuracy test = 2.3008, 10.0212 \t train = 2.3009, 10.0191 \t time = 1856.95s\n",
      "Epoch 3 \t loss / accuracy test = 2.3000, 10.0312 \t train = 2.3003, 10.0278 \t time = 1838.85s\n",
      "Epoch 5 \t loss / accuracy test = 2.2968, 10.0903 \t train = 2.2974, 10.0745 \t time = 1812.64s\n",
      "Epoch 6 \t loss / accuracy test = 2.2894, 10.2941 \t train = 2.2918, 10.1981 \t time = 1837.05s\n",
      "Epoch 7 \t loss / accuracy test = 2.1977, 13.2461 \t train = 2.2496, 11.6234 \t time = 1782.43s\n",
      "Epoch 9 \t loss / accuracy test = 2.0887, 15.9822 \t train = 2.0881, 15.7338 \t time = 1767.66s\n",
      "Epoch 10 \t loss / accuracy test = 2.0532, 16.0277 \t train = 2.0635, 16.0233 \t time = 1749.00s\n",
      "Epoch 11 \t loss / accuracy test = 2.0359, 15.6118 \t train = 2.0374, 16.4383 \t time = 1856.85s\n",
      "Epoch 12 \t loss / accuracy test = 2.0103, 16.2931 \t train = 2.0133, 16.9030 \t time = 1777.52s\n",
      "Epoch 35 \t loss / accuracy test = 1.5850, 26.9649 \t train = 1.5662, 26.8651 \t time = 1784.94s\n",
      "Epoch 36 \t loss / accuracy test = 1.5810, 26.6750 \t train = 1.5698, 26.9421 \t time = 1798.74s\n",
      "Epoch 37 \t loss / accuracy test = 1.5798, 26.7487 \t train = 1.5644, 26.9433 \t time = 1806.25s\n",
      "Epoch 38 \t loss / accuracy test = 1.5817, 26.7549 \t train = 1.5622, 27.0962 \t time = 1828.26s\n",
      "Epoch 39 \t loss / accuracy test = 1.5856, 26.7675 \t train = 1.5678, 26.8723 \t time = 1845.25s\n",
      "Epoch 40 \t loss / accuracy test = 1.5911, 26.6335 \t train = 1.5635, 27.0517 \t time = 1815.32s\n",
      "Epoch 41 \t loss / accuracy test = 1.5832, 27.1471 \t train = 1.5581, 27.1198 \t time = 1873.29s\n",
      "Epoch 42 \t loss / accuracy test = 1.5747, 26.9505 \t train = 1.5678, 26.9214 \t time = 1798.91s\n",
      "Epoch 43 \t loss / accuracy test = 1.5864, 26.8322 \t train = 1.5572, 27.2286 \t time = 1808.97s\n",
      "Epoch 44 \t loss / accuracy test = 1.5734, 27.0936 \t train = 1.5590, 27.1552 \t time = 1849.54s\n",
      "Epoch 45 \t loss / accuracy test = 1.5793, 27.0249 \t train = 1.5542, 27.3095 \t time = 1852.40s\n",
      "Epoch 46 \t loss / accuracy test = 1.5712, 26.9174 \t train = 1.5650, 27.0546 \t time = 1863.54s\n",
      "Epoch 47 \t loss / accuracy test = 1.5955, 27.2318 \t train = 1.5606, 27.1271 \t time = 1791.55s\n",
      "Epoch 48 \t loss / accuracy test = 1.5784, 27.0666 \t train = 1.5565, 27.2140 \t time = 1808.64s\n",
      "Epoch 49 \t loss / accuracy test = 1.5696, 27.3863 \t train = 1.5497, 27.3195 \t time = 1865.12s\n",
      "Epoch 50 \t loss / accuracy test = 1.5698, 27.1513 \t train = 1.5543, 27.2287 \t time = 1810.74s\n",
      "Epoch 51 \t loss / accuracy test = 1.5656, 27.4930 \t train = 1.5450, 27.4694 \t time = 1788.65s\n",
      "Epoch 52 \t loss / accuracy test = 1.5673, 27.3407 \t train = 1.5470, 27.5042 \t time = 1825.29s\n",
      "Epoch 53 \t loss / accuracy test = 1.5624, 27.1988 \t train = 1.5468, 27.4745 \t time = 1815.55s\n",
      "Epoch 54 \t loss / accuracy test = 1.5632, 27.5218 \t train = 1.5480, 27.5275 \t time = 1774.79s\n",
      "Epoch 55 \t loss / accuracy test = 1.5620, 27.2530 \t train = 1.5424, 27.4391 \t time = 1844.54s\n",
      "Epoch 56 \t loss / accuracy test = 1.5623, 27.5275 \t train = 1.5448, 27.5371 \t time = 1861.58s\n",
      "Epoch 57 \t loss / accuracy test = 1.5707, 27.2142 \t train = 1.5475, 27.5375 \t time = 1788.60s\n",
      "Epoch 58 \t loss / accuracy test = 1.5642, 27.2468 \t train = 1.5483, 27.3814 \t time = 1773.88s\n",
      "Epoch 78 \t loss / accuracy test = 1.5536, 27.6330 \t train = 1.5318, 27.7377 \t time = 1736.29s\n",
      "Epoch 79 \t loss / accuracy test = 1.5514, 27.6598 \t train = 1.5293, 27.9535 \t time = 1792.58s\n",
      "Epoch 80 \t loss / accuracy test = 1.5520, 27.7529 \t train = 1.5342, 27.8168 \t time = 1732.97s\n",
      "Epoch 81 \t loss / accuracy test = 1.5504, 27.6955 \t train = 1.5404, 27.7036 \t time = 1762.58s\n",
      "Epoch 82 \t loss / accuracy test = 1.5524, 27.7091 \t train = 1.5298, 27.8222 \t time = 1773.75s\n",
      "Epoch 83 \t loss / accuracy test = 1.5531, 27.7595 \t train = 1.5205, 28.0045 \t time = 1794.32s\n",
      "Epoch 84 \t loss / accuracy test = 1.5497, 27.6032 \t train = 1.5414, 27.7553 \t time = 1797.90s\n",
      "Epoch 85 \t loss / accuracy test = 1.5500, 27.6506 \t train = 1.5358, 27.7551 \t time = 1854.91s\n",
      "Epoch 86 \t loss / accuracy test = 1.5507, 27.7403 \t train = 1.5317, 27.8255 \t time = 1855.25s\n",
      "Epoch 87 \t loss / accuracy test = 1.5514, 27.6944 \t train = 1.5351, 27.7478 \t time = 1862.42s\n",
      "Epoch 88 \t loss / accuracy test = 1.5501, 27.6592 \t train = 1.5372, 27.7901 \t time = 1854.61s\n",
      "Epoch 89 \t loss / accuracy test = 1.5507, 27.7455 \t train = 1.5244, 27.8784 \t time = 1760.37s\n",
      "Epoch 90 \t loss / accuracy test = 1.5540, 27.6339 \t train = 1.5318, 27.9012 \t time = 1859.87s\n",
      "Epoch 91 \t loss / accuracy test = 1.5507, 27.6809 \t train = 1.5337, 27.7843 \t time = 1796.41s\n",
      "Epoch 92 \t loss / accuracy test = 1.5499, 27.6622 \t train = 1.5389, 27.6702 \t time = 1864.57s\n",
      "Epoch 93 \t loss / accuracy test = 1.5506, 27.7858 \t train = 1.5270, 27.8602 \t time = 1851.41s\n",
      "Epoch 94 \t loss / accuracy test = 1.5502, 27.7518 \t train = 1.5273, 27.8329 \t time = 1865.01s\n",
      "Epoch 95 \t loss / accuracy test = 1.5505, 27.7182 \t train = 1.5339, 27.8282 \t time = 1786.88s\n",
      "Epoch 96 \t loss / accuracy test = 1.5511, 27.7274 \t train = 1.5299, 27.8445 \t time = 1796.42s\n",
      "Epoch 97 \t loss / accuracy test = 1.5508, 27.8045 \t train = 1.5345, 27.8893 \t time = 1805.91s\n",
      "Epoch 98 \t loss / accuracy test = 1.5497, 27.7096 \t train = 1.5286, 27.8640 \t time = 1838.99s\n",
      "Epoch 99 \t loss / accuracy test = 1.5514, 27.6648 \t train = 1.5304, 27.8287 \t time = 1819.59s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f1b9bcafd5cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-b97c2e2ae466>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Xtrain, ytrain, Xtest, ytest, num_epochs)\u001b[0m\n\u001b[0;32m     59\u001b[0m                      train_err[epoch], train_acc[epoch],time.time() - start_time))\n\u001b[0;32m     60\u001b[0m     return pack(train_err, train_acc, test_err, test_acc,\n\u001b[1;32m---> 61\u001b[1;33m                 network, inp, target,train_fn, test_fn)\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "model1 = train(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
