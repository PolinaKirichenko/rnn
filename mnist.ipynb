{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import time\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mnist_images(filename):\n",
    "    # Read the inputs in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8, offset=16)\n",
    "    # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "    # following the shape convention: (examples, channels, rows, columns)\n",
    "    data = data.reshape(-1, 28, 28)\n",
    "    # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "    # (Actually to range [0, 255/256], for compatibility to the version\n",
    "    # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "    return data\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    # Read the labels in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    # The labels are vectors of integers now, that's exactly what we want.\n",
    "    return data\n",
    "\n",
    "# We can now download and read the training and test set images and labels.\n",
    "X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr4AAABZCAYAAADYWSdmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZAUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe6b6e8fa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((60000, 784))\n",
    "X_test = X_test.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.zeros((y_train.size, 10))\n",
    "a[np.arange(y_train.size), y_train] = 1\n",
    "y_train = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.zeros((y_test.size, 10))\n",
    "a[np.arange(y_test.size), y_test] = 1\n",
    "y_test = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "GRAD_CLIP = 10\n",
    "N_HIDDEN = 100\n",
    "SEQ_LEN = 784\n",
    "TRAIN_SIZE = 60000\n",
    "TEST_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numerically stable log-softmax with crossentropy\n",
    "def logsoftmax(x):\n",
    "    xdev = x-x.max(1,keepdims=True)\n",
    "    lsm = xdev - T.log(T.sum(T.exp(xdev),axis=1,keepdims=True))\n",
    "    return lsm\n",
    "\n",
    "# cross-entropy\n",
    "# ys are indices of chars, x is a matrix (? BATCH_SIZE * SEQ_LEN, VOCAB_SIZE)\n",
    "def lsmCE(x,y):\n",
    "    print(T.shape(x), T.shape(y))\n",
    "    return -T.clip(x,-20,0)[T.arange(y.shape[0]), y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Технические вещи\n",
    "\n",
    "# Вспомогательная функция для запаковки результата обучения \n",
    "def pack(network, inp, target, train_err, test_err, train_acc, test_acc, train_fn, test_fn):\n",
    "    return {'network':network,\n",
    "            'inp':inp,\n",
    "            'target':target,\n",
    "            \n",
    "            'train_err':train_err,\n",
    "            'test_err':test_err,\n",
    "            'train_acc':train_acc, \n",
    "            'test_acc':test_acc, \n",
    "            'train_fn':train_fn, \n",
    "            'test_fn':test_fn\n",
    "            } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_posdef_w(n):\n",
    "    # A = 1/N R^T * R, wehere R is standard normal\n",
    "    # A is positive definite\n",
    "    # W = (A + I) / max(spec(A + I))\n",
    "    # spec(W) = {1, l_i < 1}\n",
    "    \n",
    "    R = np.random.normal(size=(n, n))\n",
    "    A = 1 / N_HIDDEN * np.dot(R.T, R)\n",
    "    eig, _ = np.linalg.eig(A + np.eye(n))\n",
    "    e = max(eig)\n",
    "    W = (A + np.eye(n)) / e\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network_np(inp, num_epochs=NUM_EPOCHS):\n",
    "    # Network from paper \"Improving performance of RNN with ReLU\"\n",
    "    # W_hh is initialized as pos. def. with max eigenvalue 1\n",
    "    \n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, SEQ_LEN, 1), input_var=inp)\n",
    "\n",
    "    alpha = np.sqrt(2) * np.exp(1.2 / (max(N_HIDDEN, 6)))\n",
    "    l_rnn = lasagne.layers.RecurrentLayer(\n",
    "        l_in, N_HIDDEN,\n",
    "        W_in_to_hid=lasagne.init.Normal(std=alpha / N_HIDDEN, mean=0.0),\n",
    "        W_hid_to_hid=init_posdef_w(N_HIDDEN),\n",
    "        learn_init=True,\n",
    "        only_return_final=True,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(l_rnn, num_units=10,\n",
    "                                      W=lasagne.init.GlorotNormal(),\n",
    "                                      nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network_identity(inp, num_epochs=NUM_EPOCHS):\n",
    "    # Network from paper \"Simple way to initialize RNN\",\n",
    "    # W_hh is initialized as identity matrix\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, SEQ_LEN, 1), input_var=inp)\n",
    "\n",
    "    l_rnn = lasagne.layers.RecurrentLayer(\n",
    "        l_in, N_HIDDEN,\n",
    "        W_in_to_hid=lasagne.init.Normal(1e-3, mean=0.0),\n",
    "        W_hid_to_hid=np.eye(N_HIDDEN),\n",
    "        learn_init=True,\n",
    "        only_return_final=True,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    l_out = lasagne.layers.DenseLayer(l_rnn, num_units=10,\n",
    "                                      W=lasagne.init.GlorotNormal(),\n",
    "                                      nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network_gaus(inp, num_epochs=NUM_EPOCHS):\n",
    "    # W_hh is initialized from standard normal\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, SEQ_LEN, 1), input_var=inp)\n",
    "\n",
    "    l_rnn = lasagne.layers.RecurrentLayer(\n",
    "        l_in, N_HIDDEN,\n",
    "        W_in_to_hid=lasagne.init.GlorotNormal(),\n",
    "        W_hid_to_hid=lasagne.init.Normal(),\n",
    "        learn_init=True,\n",
    "        only_return_final=True,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    l_out = lasagne.layers.DenseLayer(l_rnn, num_units=10,\n",
    "                                      W=lasagne.init.GlorotNormal(),\n",
    "                                      nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a network with particular initialization for parameters\n",
    "def create_network(build_fn):\n",
    "    print(\"Building network ...\")\n",
    "    inp = T.tensor3('input', dtype='float64')\n",
    "    # Matrix of size BATCH_SIZE * 10 to match network's output\n",
    "    target_values = T.matrix('target_output', dtype='float64')\n",
    "    network = build_fn(inp)\n",
    "    print(\"The network has {} params\".format(lasagne.layers.count_params(network)))\n",
    "    return (network, inp, target_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_sgd(network, inp, target_values, Xtrain, ytrain, Xtest, ytest, filename, lr, num_epochs=NUM_EPOCHS):\n",
    "    num_batches = TRAIN_SIZE // BATCH_SIZE\n",
    "    train_err=np.zeros(num_epochs)\n",
    "    train_acc=np.zeros(num_epochs)\n",
    "    test_err=np.zeros(num_epochs)\n",
    "    test_acc=np.zeros(num_epochs)\n",
    "    \n",
    "    network_output = lasagne.layers.get_output(network)\n",
    "    cr_ent = T.mean(lasagne.objectives.categorical_crossentropy(network_output, target_values))\n",
    "    acc_score = lasagne.objectives.categorical_accuracy(\n",
    "        network_output, target_values).mean(dtype=theano.config.floatX) * 100\n",
    "    \n",
    "    # Retrieve all parameters from the network\n",
    "    all_params = lasagne.layers.get_all_params(network)\n",
    "    print(\"Computing updates ...\")\n",
    "    all_grads = T.grad(cr_ent, all_params)\n",
    "    scaled_grads, grad_norm = lasagne.updates.total_norm_constraint(all_grads, GRAD_CLIP, return_norm=True)\n",
    "    lr_var = theano.shared(lr)\n",
    "    updates = lasagne.updates.sgd(scaled_grads, all_params, learning_rate=lr_var)\n",
    "\n",
    "    print(\"Compiling functions ...\")    \n",
    "    train_fn = theano.function([inp, target_values], [cr_ent, acc_score, grad_norm],\n",
    "                               updates=updates, allow_input_downcast=True)\n",
    "    test_fn = theano.function([inp, target_values], [cr_ent, acc_score], allow_input_downcast=True)\n",
    "    get_output = theano.function([inp], network_output, allow_input_downcast=True)\n",
    "    \n",
    "    train_idxs = np.arange(TRAIN_SIZE)\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for batch in range(num_batches):\n",
    "            idx = train_idxs[BATCH_SIZE * batch : BATCH_SIZE * (batch + 1)]            \n",
    "            err, acc, norm = train_fn(Xtrain[idx, :, np.newaxis], ytrain[idx, :])\n",
    "            train_err[epoch] += err\n",
    "            train_acc[epoch] += acc\n",
    "            \n",
    "            if batch == num_batches // 4:\n",
    "                print(\"25%\", end=\" \")\n",
    "            if batch == num_batches // 2:\n",
    "                print(\"50%\", end=\" \")\n",
    "            if batch == num_batches // 4 * 3:\n",
    "                print(\"75%\")\n",
    "            \n",
    "        train_err[epoch] /= num_batches\n",
    "        train_acc[epoch] /= num_batches\n",
    "        \n",
    "        terr, tacc = test_fn(Xtest[:, :, np.newaxis], ytest)\n",
    "        test_err[epoch] = terr\n",
    "        test_acc[epoch] = tacc\n",
    "        \n",
    "        print(\"Epoch {} loss / acc test = {:.4f}, {:.4f} \\t\"\n",
    "              \"train = {:.4f}, {:.4f} norm = {:.4f} \\t time = {:.2f}s\".\n",
    "              format(epoch, test_err[epoch], test_acc[epoch], \n",
    "                     train_err[epoch], train_acc[epoch], norm.item(), time.time() - start_time), end=\"\\t\")\n",
    "    \n",
    "    np.save(file=filename, arr=lasagne.layers.get_all_param_values(network))\n",
    "    return pack(network, inp, target_values, train_err, test_err, train_acc, test_acc, train_fn, test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "The network has 11310 params\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "25% 50% 75%\n",
      "Epoch 0 loss / acc test = 2.3023, 11.3500 \ttrain = 2.3027, 11.1517 norm = 0.2568 \t time = 580.11s\t25% 50% 75%\n",
      "Epoch 1 loss / acc test = 2.3021, 11.3500 \ttrain = 2.3022, 11.2367 norm = 0.2647 \t time = 580.26s\t25% 50% 75%\n",
      "Epoch 2 loss / acc test = 2.3018, 11.3500 \ttrain = 2.3019, 11.2383 norm = 0.2657 \t time = 581.79s\t25% 50% 75%\n",
      "Epoch 3 loss / acc test = 2.3016, 11.3700 \ttrain = 2.3017, 11.2417 norm = 0.2507 \t time = 582.40s\t25% 50% 75%\n",
      "Epoch 4 loss / acc test = 2.3013, 11.9500 \ttrain = 2.3014, 11.7017 norm = 0.2568 \t time = 575.96s\t25% 50% 75%\n",
      "Epoch 5 loss / acc test = 2.3010, 11.9800 \ttrain = 2.3011, 12.0217 norm = 0.2803 \t time = 577.58s\t25% 50% 75%\n",
      "Epoch 6 loss / acc test = 2.3008, 11.9900 \ttrain = 2.3008, 12.0650 norm = 0.2666 \t time = 577.94s\t25% 50% 75%\n",
      "Epoch 7 loss / acc test = 2.3006, 12.0100 \ttrain = 2.3006, 12.1017 norm = 0.2806 \t time = 581.46s\t25% 50% 75%\n",
      "Epoch 8 loss / acc test = 2.3003, 12.0300 \ttrain = 2.3003, 12.1450 norm = 0.2762 \t time = 576.69s\t25% 50% 75%\n",
      "Epoch 9 loss / acc test = 2.2999, 12.0700 \ttrain = 2.2998, 12.2083 norm = 0.2837 \t time = 583.91s\t"
     ]
    }
   ],
   "source": [
    "net, inp, tar = create_network(build_network_np)\n",
    "nprnn1 = train_sgd(net, inp, tar, X_train, y_train, X_test, y_test, \"nprnn1\", 1e-4, num_epochs=10)\n",
    "np.save(\"acc1\", nprnn1[\"test_acc\"])\n",
    "np.save(\"err1\", nprnn1[\"test_err\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss = []\n",
    "# acc = []\n",
    "# loss.append(nprnn1[\"test_err\"])\n",
    "# acc.append(nprnn1[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "25% 50% 75%\n",
      "Epoch 0 loss / acc test = 2.2993, 12.1000 \ttrain = 2.2993, 12.2500 norm = 0.2828 \t time = 670.16s\t25% 50% 75%\n",
      "Epoch 1 loss / acc test = 2.2983, 12.1100 \ttrain = 2.2984, 12.2983 norm = 0.2835 \t time = 677.27s\t25% 50% 75%\n",
      "50% 75%\n",
      "Epoch 8 loss / acc test = 2.0871, 18.1000 \ttrain = 2.0360, 23.0950 norm = 13.1145 \t time = 640.36s\t25% 50% 75%\n",
      "Epoch 9 loss / acc test = 2.0835, 19.9500 \ttrain = 2.0214, 23.9917 norm = 19.9003 \t time = 636.89s\t25% 50% 75%\n",
      "Epoch 10 loss / acc test = 2.1215, 19.6900 \ttrain = 2.0096, 24.6000 norm = 28.7010 \t time = 638.69s\t25% 50% 75%\n",
      "Epoch 11 loss / acc test = 2.1448, 19.1400 \ttrain = 2.0001, 24.8067 norm = 63.6304 \t time = 645.02s\t25% 50% 75%\n",
      "Epoch 12 loss / acc test = 2.2513, 18.6900 \ttrain = 1.9928, 24.7100 norm = 119.3084 \t time = 635.18s\t25% 50% 75%\n",
      "Epoch 13 loss / acc test = 1.9589, 27.1900 \ttrain = 1.9851, 24.8100 norm = 274.9250 \t time = 634.92s\t25% 50% 75%\n",
      "Epoch 14 loss / acc test = 1.9629, 27.0800 \ttrain = 1.9690, 24.9150 norm = 377.5712 \t time = 637.81s\t25% 50% 75%\n",
      "Epoch 15 loss / acc test = 1.9620, 27.0600 \ttrain = 1.9541, 25.1000 norm = 397.6842 \t time = 638.59s\t25% 50% 75%\n",
      "Epoch 16 loss / acc test = 1.9210, 28.0700 \ttrain = 1.9409, 25.3833 norm = 187.7956 \t time = 633.61s\t25% 50% 75%\n",
      "Epoch 17 loss / acc test = 1.9272, 27.9900 \ttrain = 1.9239, 25.7350 norm = 249.9447 \t time = 639.31s\t25% 50% 75%\n",
      "Epoch 18 loss / acc test = 1.9031, 26.7800 \ttrain = 1.9081, 26.1417 norm = 166.5355 \t time = 647.37s\t25% 50% 75%\n",
      "Epoch 19 loss / acc test = 1.8995, 29.0800 \ttrain = 1.8925, 26.4533 norm = 161.1501 \t time = 637.77s\t25% 50% 75%\n",
      "Epoch 20 loss / acc test = 1.9121, 29.9600 \ttrain = 1.8751, 27.5883 norm = 219.7539 \t time = 645.27s\t25% 50% 75%\n",
      "Epoch 21 loss / acc test = 1.8815, 29.5300 \ttrain = 1.8534, 28.9350 norm = 111.6564 \t time = 639.11s\t25% 50% 75%\n",
      "Epoch 22 loss / acc test = 1.8668, 27.5500 \ttrain = 1.8190, 31.4467 norm = 99.2519 \t time = 639.88s\t"
     ]
    }
   ],
   "source": [
    "nprnn1 = train_sgd(nprnn1[\"network\"], nprnn1[\"inp\"], nprnn1[\"target\"],\n",
    "                X_train, y_train, X_test, y_test, \"nprnn1\", 1e-4, num_epochs=23)\n",
    "np.save(\"acc2\", nprnn1[\"test_acc\"])\n",
    "np.save(\"err2\", nprnn1[\"test_err\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COOL DOWN LEARNING RATE AFTER 30 EPOCHS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "The network has 11310 params\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "25% 50% 75%\n",
      "Epoch 0 loss / acc test = 1.7963, 30.8900 \ttrain = 1.7676, 32.0333 norm = 131.5668 \t time = 646.70s\t25% 50% 75%\n",
      "Epoch 1 loss / acc test = 1.7892, 31.2000 \ttrain = 1.7591, 32.2200 norm = 118.7580 \t time = 653.11s\t25% 50% 75%\n",
      "Epoch 2 loss / acc test = 1.7819, 31.4100 \ttrain = 1.7509, 32.3867 norm = 121.6389 \t time = 643.35s\t25% 50% 75%\n",
      "Epoch 3 loss / acc test = 1.7758, 31.5900 \ttrain = 1.7429, 32.6017 norm = 104.8153 \t time = 641.58s\t25% 50% 75%\n",
      "Epoch 4 loss / acc test = 1.7693, 31.9200 \ttrain = 1.7359, 32.8600 norm = 95.4105 \t time = 651.29s\t25% 50% 75%\n",
      "Epoch 5 loss / acc test = 1.7649, 32.1100 \ttrain = 1.7302, 32.9667 norm = 85.1824 \t time = 642.02s\t25% 50% 75%\n",
      "Epoch 6 loss / acc test = 1.7618, 32.2500 \ttrain = 1.7260, 33.0533 norm = 76.8551 \t time = 638.09s\t25% 50% 75%\n",
      "Epoch 7 loss / acc test = 1.7585, 32.2200 \ttrain = 1.7228, 33.0750 norm = 69.2033 \t time = 642.42s\t25% 50% 75%\n",
      "Epoch 8 loss / acc test = 1.7558, 32.2500 \ttrain = 1.7202, 33.0967 norm = 64.7727 \t time = 639.98s\t25% 50% 75%\n",
      "Epoch 9 loss / acc test = 1.7533, 32.2000 \ttrain = 1.7180, 33.1050 norm = 62.1306 \t time = 637.18s\t25% 50% 75%\n",
      "Epoch 10 loss / acc test = 1.7509, 32.2600 \ttrain = 1.7159, 33.1350 norm = 57.5830 \t time = 650.36s\t25% 50% 75%\n",
      "Epoch 11 loss / acc test = 1.7488, 32.2600 \ttrain = 1.7140, 33.1367 norm = 56.0970 \t time = 639.17s\t25% 50% 75%\n",
      "Epoch 12 loss / acc test = 1.7472, 32.3000 \ttrain = 1.7122, 33.1200 norm = 55.5834 \t time = 639.70s\t25% 50% 75%\n",
      "Epoch 13 loss / acc test = 1.7450, 32.3700 \ttrain = 1.7105, 33.1500 norm = 56.0397 \t time = 646.37s\t25% 50% 75%\n",
      "Epoch 14 loss / acc test = 1.7425, 32.3900 \ttrain = 1.7088, 33.1650 norm = 57.1725 \t time = 644.25s\t25% 50% 75%\n",
      "Epoch 15 loss / acc test = 1.7403, 32.4200 \ttrain = 1.7071, 33.2117 norm = 57.9989 \t time = 641.03s\t25% 50% 75%\n",
      "Epoch 16 loss / acc test = 1.7381, 32.3900 \ttrain = 1.7054, 33.2283 norm = 60.3541 \t time = 638.13s\t25% 50% 75%\n",
      "Epoch 17 loss / acc test = 1.7361, 32.3300 \ttrain = 1.7037, 33.2450 norm = 56.2612 \t time = 637.73s\t25% 50% 75%\n",
      "Epoch 18 loss / acc test = 1.7341, 32.2500 \ttrain = 1.7019, 33.2500 norm = 55.7966 \t time = 640.89s\t25% 50% 75%\n",
      "Epoch 19 loss / acc test = 1.7312, 32.1500 \ttrain = 1.7000, 33.2483 norm = 57.7137 \t time = 637.89s\t25% 50% 75%\n",
      "Epoch 20 loss / acc test = 1.7286, 31.9500 \ttrain = 1.6980, 33.2433 norm = 62.6890 \t time = 639.83s\t25% 50% 75%\n",
      "Epoch 21 loss / acc test = 1.7262, 31.8400 \ttrain = 1.6959, 33.2400 norm = 66.7622 \t time = 641.55s\t25% 50% 75%\n",
      "Epoch 22 loss / acc test = 1.7225, 31.8200 \ttrain = 1.6934, 33.2400 norm = 70.4202 \t time = 650.24s\t25% 50% 75%\n",
      "Epoch 23 loss / acc test = 1.7186, 31.8200 \ttrain = 1.6908, 33.2700 norm = 91.1815 \t time = 637.60s\t25% 50% 75%\n",
      "Epoch 24 loss / acc test = 1.7141, 31.8800 \ttrain = 1.6877, 33.2233 norm = 99.9201 \t time = 645.36s\t25% 50% 75%\n",
      "Epoch 25 loss / acc test = 1.7086, 31.8200 \ttrain = 1.6842, 33.2467 norm = 111.2544 \t time = 645.43s\t25% 50% 75%\n",
      "Epoch 26 loss / acc test = 1.7034, 31.9600 \ttrain = 1.6799, 33.3100 norm = 117.4139 \t time = 641.43s\t25% 50% 75%\n",
      "Epoch 27 loss / acc test = 1.6975, 32.5200 \ttrain = 1.6757, 33.5967 norm = 110.3925 \t time = 639.88s\t25% 50% 75%\n",
      "Epoch 28 loss / acc test = 1.6887, 33.5000 \ttrain = 1.6721, 34.1233 norm = 141.3137 \t time = 640.65s\t25% 50% 75%\n",
      "Epoch 29 loss / acc test = 1.6820, 34.9500 \ttrain = 1.6744, 34.9367 norm = 148.6024 \t time = 638.24s\t25% 50% 75%\n",
      "Epoch 30 loss / acc test = 1.6788, 35.7800 \ttrain = 1.6753, 35.8417 norm = 156.9511 \t time = 638.55s\t25% 50% 75%\n",
      "Epoch 31 loss / acc test = nan, 9.8000 \ttrain = nan, 29.9800 norm = nan \t time = 620.23s\t25% 50% 75%\n",
      "Epoch 32 loss / acc test = nan, 9.8000 \ttrain = nan, 9.8717 norm = nan \t time = 586.99s\t"
     ]
    }
   ],
   "source": [
    "net, inp, tar = create_network(build_network_np)\n",
    "weights = np.load(\"nprnn1.npy\")\n",
    "lasagne.layers.set_all_param_values(net, weights)\n",
    "nprnn1 = train_sgd(net, inp, tar, X_train, y_train, X_test, y_test, \"nprnn1\", 1e-5, num_epochs=33)\n",
    "np.save(\"acc3\", nprnn1[\"test_acc\"])\n",
    "np.save(\"err3\", nprnn1[\"test_err\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COOL DOWN LEARNING RATE AFTER 60 EPOCHS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
